# Architectural Considerations

Hard-won insights from building the Knowsee Platform. Read before extending.

---

## LangGraph Streaming Architecture

### Blocking Calls Create Dead Zones

The streaming architecture requires every node in the graph to complete quickly or emit events continuously. Blocking LLM calls in post-processing nodes (like `generate_output`) create "dead zones" where the frontend appears stuck.

```
Timeline with blocking call:
[text streams] -> [2-5s silence] -> [suggestions appear]
                   ^ user thinks it's done, sends new message
```

**Solution**: Use rule-based alternatives for post-processing. Trade intelligence for responsiveness.

```python
# Bad: Blocks stream for 2-5 seconds
async def _generate_output_node(state):
    suggestions = await llm.ainvoke(suggestion_prompt)  # Blocking!
    return {"suggestions": suggestions}

# Good: Instant, no blocking
def _generate_suggestions(state) -> list[str]:
    if "trend" in user_query:
        return ["Compare with last year", "Show weekly breakdown", ...]
```

### Subgraph Event Propagation

When adding a compiled subgraph as a node, add it **directly** - not wrapped in a function that calls `ainvoke()`. Only direct addition allows `astream_events()` to traverse into the subgraph.

```python
# Bad: Events from subgraph are not visible
async def data_analyst_wrapper(state):
    return await data_analyst_graph.ainvoke(state)  # Events lost!
graph_builder.add_node("data_analyst", data_analyst_wrapper)

# Good: Events propagate through subgraph
graph_builder.add_node("data_analyst", data_analyst_graph)
```

### Router Node Filtering

Without filtering, intent classification output leaks into the user-facing stream. Always check `langgraph_node` metadata:

```python
if event_type == "on_chat_model_stream":
    langgraph_node = metadata.get("langgraph_node", "")
    if langgraph_node == "router":
        continue  # Don't stream router output to user
```

---

## Gemini 2.5 Flash Specifics

### Thinking Token Overhead

Gemini 2.5 Flash uses internal "thinking" tokens (~50-100) that count against `max_output_tokens`. A setting of `max_output_tokens=10` for a single-word classification will fail silently - the model exhausts its budget on thinking before producing output.

```python
# Bad: Returns empty string, defaults to fallback
_router_llm = ChatVertexAI(
    model="gemini-2.5-flash",
    max_output_tokens=10,  # Too low!
)

# Good: Accounts for thinking overhead
_router_llm = ChatVertexAI(
    model="gemini-2.5-flash",
    max_output_tokens=128,  # ~100 thinking + actual output
)
```

**Symptoms**: Empty responses, unexpected fallbacks, `"Unexpected router response"` logs.

---

## LLM Context Awareness

### Date Injection Required

LLMs have knowledge cutoffs and no innate sense of "now". Without explicit date injection, the model may claim 2024 is "ongoing" when it's December 2025.

```python
# System prompt template
SYSTEM_PROMPT_TEMPLATE = """You are a Data Analyst assistant.

**Current date: {current_date}**

## Capabilities
...
"""

# Inject at runtime
current_date = datetime.now(timezone.utc).strftime("%Y-%m-%d")
system_prompt = SYSTEM_PROMPT_TEMPLATE.format(current_date=current_date)
```

### Preventing Unwanted Output

LLMs may include JSON arrays, suggestions, or formatting you didn't ask for. Explicit negative instructions are more effective than positive-only guidance:

```python
# Weak: Model ignores
"Suggestions are generated separately."

# Strong: Model complies
"NEVER include follow-up suggestions - they are generated by a separate system.
NEVER output JSON arrays of suggestions - this is handled automatically."
```

---

## Tool Output Parsing

### ToolMessage Content Structure

The `generate_output` node must extract artifacts from `ToolMessage.content`. Content is a JSON string that needs parsing:

```python
for msg in messages:
    if not isinstance(msg, ToolMessage):
        continue

    try:
        parsed = json.loads(msg.content)
        if "rows" in parsed and "columns" in parsed:
            query_results = parsed["rows"]
        if "output_type" in parsed:
            chart_config = parsed.get("config")
    except json.JSONDecodeError:
        continue  # Non-JSON tool outputs are silently ignored
```

**Gotcha**: Non-JSON tool outputs (plain text responses) are dropped without logging. Ensure all tools return structured JSON for artifact extraction.

---

## Vercel AI SDK Protocol

### Event Type Naming

Backend emits nested data events. The AI SDK parser adds a `data-` prefix:

```python
# Backend emits
{"type": "data", "data": [{"type": "chartStart", "value": {...}}]}

# Frontend receives as
streamPart.type === "data-chartStart"
```

### Event Ordering

The text stream lifecycle must be respected:

```
start -> start-step -> text-start -> text-delta* -> text-end -> finish-step -> finish -> [DONE]
```

Missing `text-start` before `text-delta` causes deltas to be ignored by the frontend.

### Custom Data Events for Artifacts

Chart events follow a specific sequence:

```python
yield create_chart_start(chart_id, chart_type)   # Opens artifact panel
yield create_chart_data(chart_id, query_results) # Sends data array
yield create_chart_config(chart_id, config)      # Sends axis/series config
yield create_chart_end(chart_id)                 # Marks complete
```

---

## TypeScript/Python Type Boundaries

### LangChain Message Content

`msg.content` in LangChain is typed as `str | list[str | dict]`. Always narrow before string operations:

```python
# Bad: Type error
content = msg.content.lower()

# Good: Type narrowed
content = msg.content if isinstance(msg.content, str) else ""
```

### Async Callback Signatures

When wrapping async callbacks in React, preserve the Promise return type:

```typescript
// Bad: Returns void, TypeScript error
sendMessage={(msg) => {
  setSuggestions([]);
  sendMessage(msg);  // Promise not returned
}}

// Good: Returns Promise<void>
sendMessage={async (msg) => {
  setSuggestions([]);
  await sendMessage(msg);
}}
```

---

## Frontend Artifact System

### Metadata is Ephemeral

Artifact metadata (chart type, config, axis mappings) lives in SWR memory only. It's **not persisted** to the database. Reloading the page loses all metadata - only the raw `content` string survives via the Document table.

### Kind Matching is Exact

`artifact.kind` must exactly match one of the `artifactDefinitions` entries. Typos cause silent failures where the artifact panel opens but shows nothing.

### Version History Limitations

Document versions store `content` string only. Complex metadata like chart configurations, series colours, or axis mappings are not versioned. Undo/redo only affects text content.

---

## Integration Boundaries

### Frontend API Route Proxy Gap

The Next.js API route at `/api/chat` only forwards `text-delta` events from the backend. Tool events, chart events, and suggestions are **not forwarded**. This breaks artifact functionality when using the proxy.

For full functionality, the frontend must connect directly to the backend streaming endpoint, or the proxy must be extended to forward all event types.

### No User Context to Backend

The backend receives no authentication or user context. Session validation happens in the frontend API route - the backend is stateless. This limits:
- Per-user rate limiting at the agent level
- Personalised tool behaviour
- User-specific BigQuery project access

---

## BigQuery Tool Safety

### Query Validation

SQL validation uses regex to detect dangerous keywords. This happens **at the tool level**, not the LLM level - it cannot be bypassed by prompt injection:

```python
dangerous_keywords = [
    "INSERT", "UPDATE", "DELETE", "DROP", "CREATE",
    "ALTER", "TRUNCATE", "MERGE", "GRANT", "REVOKE",
]
```

### Result Size Limits

Hard limits prevent runaway costs:
- Maximum 1000 rows per query (`MAX_QUERY_ROWS`)
- 30-second timeout (`QUERY_TIMEOUT`)
- Results are truncated, not paginated

Large result sets return partial data with a `truncated: true` flag.

---

## Extension Patterns

### Adding a New Agent

1. Create `backend/src/agents/{name}/state.py` with `TypedDict` using `add_messages` reducer
2. Define tools in `backend/src/agents/{name}/tools.py` with `@tool` decorator
3. Build ReAct graph in `backend/src/agents/{name}/agent.py`
4. Add keywords to `ROUTER_PROMPT` for intent classification
5. Add routing case in `graph.py`

### Adding a New Tool

1. Define function with `@tool` decorator and detailed docstring
2. Return structured dict with error handling (`{"error": "message"}` pattern)
3. Add to `ALL_TOOLS` list - automatically bound to agent

### Adding a New Artifact

1. Define TypeScript metadata type
2. Create `frontend/artifacts/{name}/client.tsx` extending `Artifact<Kind, Metadata>`
3. Implement `initialize`, `onStreamPart`, `content` methods
4. Add to `artifactDefinitions` array in `artifact.tsx`
5. Create backend protocol helpers in `protocol.py`

---

## Debugging Checklist

| Symptom | Likely Cause |
|---------|--------------|
| Empty router response | `max_output_tokens` too low for Gemini thinking |
| JSON in text response | System prompt not explicitly forbidding it |
| Charts not rendering | `chart_config` not extracted from tool output |
| Blank periods during stream | Blocking LLM call in post-processing node |
| Tool events not visible | Subgraph wrapped in function instead of direct add |
| Suggestions not appearing | `data-suggestions` event not handled in chat.tsx |
| Date confusion | System prompt missing `{current_date}` |
| Type errors on msg.content | Missing isinstance narrowing for str | list |

---

## Critical Assessment

**Strengths**:
- Clean separation between LangGraph backend and Next.js frontend
- Well-structured Vercel AI SDK protocol implementation
- Solid ReAct agent pattern with safety-first tool design

**Gaps**:
- Frontend proxy incomplete - doesn't forward artifact events
- Metadata persistence missing - chart configs lost on reload
- Error handling minimal - malformed events silently dropped
- No user context to backend - limits personalisation
- Hardcoded 5-iteration limit may be too restrictive

**Recommendations**:
1. Extend frontend proxy to forward all event types
2. Add metadata persistence to Document model
3. Implement budget-based iteration limits instead of hard count
4. Add structured logging for dropped events
5. Consider passing user context via headers for personalisation
